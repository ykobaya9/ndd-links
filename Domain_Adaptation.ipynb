{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import color, transform, img_as_ubyte\n",
    "\n",
    "train_mat = scipy.io.loadmat('/Users/yutakobayashi/Desktop/FA2020/NDD/train_32x32.mat')\n",
    "test_mat = scipy.io.loadmat('/Users/yutakobayashi/Desktop/FA2020/NDD/test_32x32.mat')\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_mat.get('X')\n",
    "test_x = test_mat.get('X')\n",
    "data_x_svhn = np.concatenate([train_x, test_x], axis=3)\n",
    "train_y = train_mat.get('y')\n",
    "test_y = test_mat.get('y')\n",
    "data_y_svhn = np.concatenate([train_y, test_y]).reshape(-1)\n",
    "data_x_svhn = data_x_svhn.transpose((3,0,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_mnist = np.concatenate([X_train, X_test])\n",
    "data_x_mnist = data_x_mnist.reshape((data_x_mnist.shape[0], data_x_mnist.shape[1] * data_x_mnist.shape[2]))\n",
    "data_y_mnist = np.concatenate([y_train, y_test]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_svhn_x = []\n",
    "for i in range(10000):\n",
    "    a = color.rgb2gray(data_x_svhn[i])\n",
    "    resized_svhn_x.append(img_as_ubyte(transform.resize(a, (28,28))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_svhn_processed = np.stack(resized_svhn_x)\n",
    "#print(x_svhn_processed.shape)\n",
    "x_svhn_processed = x_svhn_processed.reshape((x_svhn_processed.shape[0], x_svhn_processed.shape[1] * x_svhn_processed.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y_svhn = data_y_svhn[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proglearn.progressive_learner import ProgressiveLearner\n",
    "from proglearn.deciders import SimpleArgmaxAverage\n",
    "from proglearn.transformers import TreeClassificationTransformer, NeuralClassificationTransformer\n",
    "from proglearn.voters import TreeClassificationVoter, KNNClassificationVoter\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from itertools import product\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_data(data_x_mnist, data_y_mnist, data_x_svhn, data_y_svhn, total_cls=10):\n",
    "    x = data_x_mnist.copy()\n",
    "    y = data_y_mnist.copy()\n",
    "    x2 = data_x_svhn.copy()\n",
    "    y2 = data_y_svhn.copy()\n",
    "    idx = [np.where(y == u)[0] for u in np.unique(y)]\n",
    "    idx2 = [np.where(y2 == u)[0] for u in np.unique(y2)]\n",
    "\n",
    "    for i in range(total_cls):\n",
    "        indx = idx[i]#np.roll(idx[i],(cv-1)*100)\n",
    "        indx2 = idx2[i]\n",
    "        random.shuffle(indx)\n",
    "        random.shuffle(indx2)\n",
    "\n",
    "        if i==0:\n",
    "            train_x1 = x[indx[0:500],:]\n",
    "            train_x2 = x2[indx2[0:500],:]\n",
    "            train_y1 = y[indx[0:500]]\n",
    "            train_y2 = y2[indx2[0:500]]\n",
    "\n",
    "            test_x = x[indx[500:505],:]\n",
    "            test_y = y[indx[500:505]]\n",
    "        else:\n",
    "            train_x1 = np.concatenate((train_x1, x[indx[0:500],:]), axis=0)\n",
    "            train_x2 = np.concatenate((train_x2, x2[indx2[0:500],:]), axis=0)\n",
    "            train_y1 = np.concatenate((train_y1, y[indx[0:500]]), axis=0)\n",
    "            train_y2 = np.concatenate((train_y2, y2[indx2[0:500]]), axis=0)\n",
    "\n",
    "            test_x = np.concatenate((test_x, x[indx[500:505],:]), axis=0)\n",
    "            test_y = np.concatenate((test_y, y[indx[500:505]]), axis=0)\n",
    "\n",
    "\n",
    "    return train_x1, train_y1, train_x2, train_y2, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_experiment(train_x1, train_y1, train_x2, train_y2, test_x, test_y, ntrees, acorn=None):\n",
    "  \n",
    "    default_transformer_class = TreeClassificationTransformer\n",
    "    default_transformer_kwargs = {\"kwargs\" : {\"max_depth\" : 30}}\n",
    "\n",
    "    default_voter_class = TreeClassificationVoter\n",
    "    default_voter_kwargs = {}\n",
    "\n",
    "    default_decider_class = SimpleArgmaxAverage\n",
    "    progressive_learner = ProgressiveLearner(default_transformer_class = default_transformer_class,\n",
    "                                         default_transformer_kwargs = default_transformer_kwargs,\n",
    "                                         default_voter_class = default_voter_class,\n",
    "                                         default_voter_kwargs = default_voter_kwargs,\n",
    "                                         default_decider_class = default_decider_class)\n",
    "\n",
    "    errors = np.zeros(2)\n",
    "    \n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "\n",
    "    progressive_learner.add_task(\n",
    "        X = train_x1,\n",
    "        y = train_y1,\n",
    "        decider_kwargs = {\"classes\" : np.unique(train_y1)},\n",
    "        voter_kwargs = {\"classes\" : np.unique(train_y1)}\n",
    "    )\n",
    "\n",
    "    progressive_learner.add_transformer(\n",
    "        X = train_x2,\n",
    "        y = train_y2,\n",
    "        #decider_kwargs = {\"classes\" : np.unique(train_y2)},\n",
    "        #voter_kwargs = {\"classes\" : np.unique(train_y2)},\n",
    "        backward_task_ids = [0]\n",
    "    )\n",
    "\n",
    "    llf_single_task=progressive_learner.predict(test_x, task_id=0, transformer_ids=[0])\n",
    "    llf_task1=progressive_learner.predict(test_x, task_id=0)\n",
    "    \n",
    "    errors[0] = errors[0]+(1 - np.mean(llf_single_task == test_y))\n",
    "    errors[1] = errors[1]+(1 - np.mean(llf_task1 == test_y))\n",
    "\n",
    "    print(\"Errors: {}\".format(errors))\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_exp(data_x, data_y, data_x2, data_y2, n_trees):\n",
    "    train_x1, train_y1, train_x2, train_y2, test_x, test_y = cross_val_data(data_x, data_y, data_x2, data_y2)\n",
    "    errors = LF_experiment(train_x1, train_y1, train_x2, train_y2, test_x, test_y, n_trees, acorn=12345)\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 784)\n",
      "(50, 784)\n",
      "(50, 784)\n",
      "Errors: [0.8 0.8]\n"
     ]
    }
   ],
   "source": [
    "reps = range(1)\n",
    "n_trees=20 # Number of trees in UF\n",
    "\n",
    "for i in reps:\n",
    "    errors = run_parallel_exp(x_svhn_processed, data_y_svhn, data_x_mnist, data_y_mnist, n_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
